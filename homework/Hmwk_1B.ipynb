{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning Example Case: \n",
    "House Sale Price Prediction (like Zillow's \"zestimate\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see a line starting with \"TASK\", do that task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Click on the next cell and press shift-enter\n",
    "You will get the code in it get executed.   \n",
    "The result of last command or representation of last varible in that cell will be displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageType_NA</th>\n",
       "      <th>SaleType_COD</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_ConLI</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 238 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0   1          60         65.0     8450            7            5       2003   \n",
       "1   2          20         80.0     9600            6            8       1976   \n",
       "2   3          60         68.0    11250            7            5       2001   \n",
       "3   4          70         60.0     9550            7            5       1915   \n",
       "4   5          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea ExterQual  ... GarageType_NA SaleType_COD  \\\n",
       "0          2003       196.0        Gd  ...             0            0   \n",
       "1          1976         0.0        TA  ...             0            0   \n",
       "2          2002       162.0        Gd  ...             0            0   \n",
       "3          1970         0.0        TA  ...             0            0   \n",
       "4          2000       350.0        Gd  ...             0            0   \n",
       "\n",
       "  SaleType_CWD SaleType_Con SaleType_ConLD  SaleType_ConLI SaleType_ConLw  \\\n",
       "0            0            0              0               0              0   \n",
       "1            0            0              0               0              0   \n",
       "2            0            0              0               0              0   \n",
       "3            0            0              0               0              0   \n",
       "4            0            0              0               0              0   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  \n",
       "0             0             0            1  \n",
       "1             0             0            1  \n",
       "2             0             0            1  \n",
       "3             0             0            1  \n",
       "4             0             0            1  \n",
       "\n",
       "[5 rows x 238 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv('data/housing_processed.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Columns\n",
    "Some columns were not removed when equivalent coded ones were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterQual_Coded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gd</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gd</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gd</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ExterQual  ExterQual_Coded\n",
       "0        Gd                3\n",
       "1        TA                2\n",
       "2        Gd                3\n",
       "3        TA                2\n",
       "4        Gd                3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing[[\"ExterQual\",\"ExterQual_Coded\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering in a series\n",
    "dtypes returns a series   \n",
    "filtering series and dataframes are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(housing.dtypes==object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.dtypes[housing.dtypes==object].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"SalePrice\" in housing.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Undesired Columns\n",
    "In my case, my colleague had left above non-numeric columns in preprocessing, after creating corresponding coded versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(housing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could drop columns by name:\n",
    "housing_ml = housing.drop(columns=[\"ExterQual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or wholesale, keeping only numeric:\n",
    "housing_ml = housing.loc[:,housing.dtypes != object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(housing_ml.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Target into new Variable\n",
    "- \"SalePrice\" is the target.    \n",
    " - The value we want to predict from other values (features) for a house.  \n",
    "- Currently it is a column like the other features.   \n",
    "- Scikit-learn needs 2 variables: features (X) and target (y) to be Predicted into its own 1-D array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "- Both Pandas and scikit-learn are build on top of NumPy\n",
    "- scikit-learn can not directly work on dataframes\n",
    "- X and y data type needs to be NumPy \"ndarrays\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 222)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data as features and target\n",
    "# take \"SalePrice\" values into its own 1-D array \n",
    "sale_price = housing_ml.pop('SalePrice')\n",
    "type(sale_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 221)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pop removes the column\n",
    "# \"in place\" operation\n",
    "# now housing_ml has one less column\n",
    "housing_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sale_price.values\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See what other methods are available for ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# press tab after putting cursor after dot \".\"\n",
    "#y. #uncomment, press tab after . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n",
    "# (1460,)\n",
    "# it is equivalent to (1460)\n",
    "# means it is a 1-d array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: get ndarray version of feature dataframe put it onto variable X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_ml.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: check the shape of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 221)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: programmatically check if X and y has matching number of rows\n",
    "You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model\n",
    "Q: What would you do if you had no features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: You would always estimatate the average house price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to do much better than that.  \n",
    "We have so much data to base our decision on.   \n",
    "It can still serve us as a baseline to compare.   \n",
    "An inferior baseline could be: random in the range or max and min in training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyRegressor()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import estimator\n",
    "from sklearn.dummy import DummyRegressor\n",
    "# Instantiate estimator\n",
    "# guess the mean every single time\n",
    "mean_reg = DummyRegressor(strategy='mean')\n",
    "# fit estimator\n",
    "mean_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([180921.19589041, 180921.19589041, 180921.19589041, ...,\n",
       "       180921.19589041, 180921.19589041, 180921.19589041])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "mean_reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating The Model\n",
    "scikit-learn regressors have a score function.   \n",
    "It gives you how much better your model does compared to worst model\n",
    "Technically: what percentage of the variance has decreased over the worst model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Mean\" *is* the worst model, so its score will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a linear model \n",
    "First, let's use only one feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lf = housing_ml[['LotFrontage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.fit(X_lf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you see that it used defaults to create the estimator.   \n",
    "You could google \"LinearRegression sklearn\" and find the documentation:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "to see the options for the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_model.predict(X_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11215612336205605"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.score(X_lf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart Showing the Linear Fit\n",
    "matplotlib is the most common visualization library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using all predictors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 221)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We had 81 columns (80 features) in original dataset,\n",
    "# coded as 221 features!\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred3 = linear_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Split the data for training and testing, to use 80 percent as training data. \n",
    "use 21 as your randomization seed (so you achieve same results for us to grade). \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html \n",
    "random_state = 21 train_size = .8 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0] == y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Analyze which feature alone would give the best prediction, list the scores and RMSE errors achieved by the top 10 predictors by score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = list(housing_ml.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.61873\n",
      "Feature: 1, Score: -104.76818\n",
      "Feature: 2, Score: 82.91462\n",
      "Feature: 3, Score: 0.57011\n",
      "Feature: 4, Score: 8283.85534\n",
      "Feature: 5, Score: 5379.56249\n",
      "Feature: 6, Score: 261.54243\n",
      "Feature: 7, Score: 6.79768\n",
      "Feature: 8, Score: 30.77009\n",
      "Feature: 9, Score: 18.20556\n",
      "Feature: 10, Score: 2.11373\n",
      "Feature: 11, Score: -2.06695\n",
      "Feature: 12, Score: 18.25228\n",
      "Feature: 13, Score: 12.11180\n",
      "Feature: 14, Score: 30.51743\n",
      "Feature: 15, Score: -5.95857\n",
      "Feature: 16, Score: 36.67065\n",
      "Feature: 17, Score: 1407.35083\n",
      "Feature: 18, Score: 1238.64194\n",
      "Feature: 19, Score: 1796.26409\n",
      "Feature: 20, Score: 2394.63247\n",
      "Feature: 21, Score: -6076.78357\n",
      "Feature: 22, Score: -14240.75205\n",
      "Feature: 23, Score: 2547.03836\n",
      "Feature: 24, Score: 6356.12542\n",
      "Feature: 25, Score: 4446.39314\n",
      "Feature: 26, Score: 15.88626\n",
      "Feature: 27, Score: 10.96318\n",
      "Feature: 28, Score: 9.95645\n",
      "Feature: 29, Score: -0.21072\n",
      "Feature: 30, Score: 37.38819\n",
      "Feature: 31, Score: 34.41951\n",
      "Feature: 32, Score: 70.84911\n",
      "Feature: 33, Score: -0.14885\n",
      "Feature: 34, Score: -644.24495\n",
      "Feature: 35, Score: 69.48480\n",
      "Feature: 36, Score: 5047.79013\n",
      "Feature: 37, Score: -3150.02255\n",
      "Feature: 38, Score: 3700.82269\n",
      "Feature: 39, Score: -6216.68470\n",
      "Feature: 40, Score: 5121.67671\n",
      "Feature: 41, Score: -70.35401\n",
      "Feature: 42, Score: 530.65332\n",
      "Feature: 43, Score: 1163.18136\n",
      "Feature: 44, Score: 6083.21003\n",
      "Feature: 45, Score: 5572.63290\n",
      "Feature: 46, Score: -1733.84353\n",
      "Feature: 47, Score: 1107.18757\n",
      "Feature: 48, Score: 5331.48669\n",
      "Feature: 49, Score: -1989.50669\n",
      "Feature: 50, Score: 77.71901\n",
      "Feature: 51, Score: -618.02096\n",
      "Feature: 52, Score: -2788.90364\n",
      "Feature: 53, Score: 6786.98956\n",
      "Feature: 54, Score: 3192.35671\n",
      "Feature: 55, Score: -3682.57901\n",
      "Feature: 56, Score: 2792.62561\n",
      "Feature: 57, Score: -6300.48924\n",
      "Feature: 58, Score: -22159.88039\n",
      "Feature: 59, Score: 8891.84313\n",
      "Feature: 60, Score: 6410.31995\n",
      "Feature: 61, Score: 4695.81735\n",
      "Feature: 62, Score: 2161.89995\n",
      "Feature: 63, Score: -11509.45998\n",
      "Feature: 64, Score: 11509.45998\n",
      "Feature: 65, Score: -3953.25868\n",
      "Feature: 66, Score: 5426.40414\n",
      "Feature: 67, Score: 201.27905\n",
      "Feature: 68, Score: -1674.42452\n",
      "Feature: 69, Score: -1450.52723\n",
      "Feature: 70, Score: 7677.27352\n",
      "Feature: 71, Score: -9408.85231\n",
      "Feature: 72, Score: 3182.10603\n",
      "Feature: 73, Score: 24068.51087\n",
      "Feature: 74, Score: -24068.51087\n",
      "Feature: 75, Score: 2535.96474\n",
      "Feature: 76, Score: 12774.19047\n",
      "Feature: 77, Score: -2418.74808\n",
      "Feature: 78, Score: -15132.87994\n",
      "Feature: 79, Score: 2241.47281\n",
      "Feature: 80, Score: 5854.24212\n",
      "Feature: 81, Score: 11483.62586\n",
      "Feature: 82, Score: -17337.86798\n",
      "Feature: 83, Score: 301.38645\n",
      "Feature: 84, Score: 6784.52192\n",
      "Feature: 85, Score: 10388.36790\n",
      "Feature: 86, Score: 2297.06494\n",
      "Feature: 87, Score: -10974.94210\n",
      "Feature: 88, Score: -12328.29878\n",
      "Feature: 89, Score: 11866.80101\n",
      "Feature: 90, Score: -14409.23297\n",
      "Feature: 91, Score: -10557.00502\n",
      "Feature: 92, Score: -6026.20546\n",
      "Feature: 93, Score: -4049.02730\n",
      "Feature: 94, Score: -14800.14960\n",
      "Feature: 95, Score: -12237.22437\n",
      "Feature: 96, Score: 15237.46917\n",
      "Feature: 97, Score: -16113.17219\n",
      "Feature: 98, Score: 20546.24060\n",
      "Feature: 99, Score: 30059.67127\n",
      "Feature: 100, Score: -8641.47931\n",
      "Feature: 101, Score: -7028.99859\n",
      "Feature: 102, Score: -3650.65311\n",
      "Feature: 103, Score: -5723.55536\n",
      "Feature: 104, Score: -2996.72057\n",
      "Feature: 105, Score: 38332.92141\n",
      "Feature: 106, Score: -8759.17254\n",
      "Feature: 107, Score: 2481.39261\n",
      "Feature: 108, Score: -2625.57194\n",
      "Feature: 109, Score: -513.66136\n",
      "Feature: 110, Score: 10077.73200\n",
      "Feature: 111, Score: 3432.26284\n",
      "Feature: 112, Score: 6301.16673\n",
      "Feature: 113, Score: -18052.99755\n",
      "Feature: 114, Score: 6650.20538\n",
      "Feature: 115, Score: -10804.38435\n",
      "Feature: 116, Score: 5535.24824\n",
      "Feature: 117, Score: 39598.60367\n",
      "Feature: 118, Score: 29167.23187\n",
      "Feature: 119, Score: 29749.91162\n",
      "Feature: 120, Score: 78280.68402\n",
      "Feature: 121, Score: -194699.75415\n",
      "Feature: 122, Score: -64876.00193\n",
      "Feature: 123, Score: 41299.67639\n",
      "Feature: 124, Score: 41479.64850\n",
      "Feature: 125, Score: 5426.56013\n",
      "Feature: 126, Score: 11294.61340\n",
      "Feature: 127, Score: 4254.53082\n",
      "Feature: 128, Score: -12546.49493\n",
      "Feature: 129, Score: -8429.20942\n",
      "Feature: 130, Score: 308.04890\n",
      "Feature: 131, Score: 9499.77025\n",
      "Feature: 132, Score: 10132.63867\n",
      "Feature: 133, Score: -20252.65819\n",
      "Feature: 134, Score: -7390.20055\n",
      "Feature: 135, Score: -4066.83640\n",
      "Feature: 136, Score: 2925.55853\n",
      "Feature: 137, Score: 8843.67878\n",
      "Feature: 138, Score: -13435.43953\n",
      "Feature: 139, Score: -13765.69058\n",
      "Feature: 140, Score: -11179.60844\n",
      "Feature: 141, Score: -10044.03602\n",
      "Feature: 142, Score: -4567.30870\n",
      "Feature: 143, Score: 52992.08326\n",
      "Feature: 144, Score: -584554.93445\n",
      "Feature: 145, Score: 63900.92896\n",
      "Feature: 146, Score: 135492.39873\n",
      "Feature: 147, Score: 106326.19795\n",
      "Feature: 148, Score: 40949.37778\n",
      "Feature: 149, Score: 58997.73425\n",
      "Feature: 150, Score: 41927.69496\n",
      "Feature: 151, Score: 136960.60181\n",
      "Feature: 152, Score: 14860.91502\n",
      "Feature: 153, Score: -7353.74872\n",
      "Feature: 154, Score: -11916.08246\n",
      "Feature: 155, Score: 17492.20953\n",
      "Feature: 156, Score: 945.84523\n",
      "Feature: 157, Score: 1810.61831\n",
      "Feature: 158, Score: -340.64968\n",
      "Feature: 159, Score: -45516.38053\n",
      "Feature: 160, Score: 9418.12871\n",
      "Feature: 161, Score: -4966.92867\n",
      "Feature: 162, Score: 8072.77427\n",
      "Feature: 163, Score: 11588.01086\n",
      "Feature: 164, Score: -1846.53802\n",
      "Feature: 165, Score: 430.51259\n",
      "Feature: 166, Score: 7321.31356\n",
      "Feature: 167, Score: -8693.62497\n",
      "Feature: 168, Score: 4236.78217\n",
      "Feature: 169, Score: 10869.33310\n",
      "Feature: 170, Score: -4088.49861\n",
      "Feature: 171, Score: 945.84523\n",
      "Feature: 172, Score: 13005.24376\n",
      "Feature: 173, Score: 141.72766\n",
      "Feature: 174, Score: 25969.86564\n",
      "Feature: 175, Score: -2097.72910\n",
      "Feature: 176, Score: -26621.30565\n",
      "Feature: 177, Score: 1407.70835\n",
      "Feature: 178, Score: -9919.23596\n",
      "Feature: 179, Score: -8327.63356\n",
      "Feature: 180, Score: 5400.74530\n",
      "Feature: 181, Score: 2604.54897\n",
      "Feature: 182, Score: -4833.77232\n",
      "Feature: 183, Score: -1801.20560\n",
      "Feature: 184, Score: -8817.74451\n",
      "Feature: 185, Score: -923.22795\n",
      "Feature: 186, Score: 5940.30129\n",
      "Feature: 187, Score: 5601.87677\n",
      "Feature: 188, Score: 1868.49419\n",
      "Feature: 189, Score: 3898.15692\n",
      "Feature: 190, Score: 5784.92844\n",
      "Feature: 191, Score: 16131.01533\n",
      "Feature: 192, Score: 4813.09043\n",
      "Feature: 193, Score: -32495.68531\n",
      "Feature: 194, Score: 8463.54543\n",
      "Feature: 195, Score: 5278.19460\n",
      "Feature: 196, Score: 2348.35957\n",
      "Feature: 197, Score: 5849.38448\n",
      "Feature: 198, Score: -36764.32427\n",
      "Feature: 199, Score: 14824.84019\n",
      "Feature: 200, Score: -37.61560\n",
      "Feature: 201, Score: -1589.59340\n",
      "Feature: 202, Score: 159.37371\n",
      "Feature: 203, Score: 4488.32028\n",
      "Feature: 204, Score: -3020.48499\n",
      "Feature: 205, Score: -19860.74865\n",
      "Feature: 206, Score: -3803.79582\n",
      "Feature: 207, Score: 4071.57955\n",
      "Feature: 208, Score: -2955.82238\n",
      "Feature: 209, Score: -1394.98711\n",
      "Feature: 210, Score: 1205.73637\n",
      "Feature: 211, Score: 22738.03803\n",
      "Feature: 212, Score: -11665.13521\n",
      "Feature: 213, Score: 6425.23682\n",
      "Feature: 214, Score: 16716.50837\n",
      "Feature: 215, Score: 3767.24513\n",
      "Feature: 216, Score: -6865.88680\n",
      "Feature: 217, Score: -13520.46957\n",
      "Feature: 218, Score: 17346.77385\n",
      "Feature: 219, Score: -1599.29147\n",
      "Feature: 220, Score: -10604.98111\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  16,  14,  12,   9,  25,  13,   6,   2,  26,   3,  11,  51,\n",
       "        22,  34, 210,  27,   7, 218,  23,   5,  19,  44,  36, 164,   8,\n",
       "        40,  47, 195,  21,  28,  41,   0,  49,  45,  29,  58,  38,  43,\n",
       "        95,  10,  52,   1, 139,  48, 220,  65,  62,  77, 187,  50,  17,\n",
       "       108,  24, 155,  56,  75,  90, 130, 109,  35, 186,  46,  20,  89,\n",
       "       177, 104, 102,  55,  81,  68, 141, 204, 103,  99,  79,  15, 190,\n",
       "       180,  37,  59, 206,  39, 188,  57, 145, 152, 182,  94, 147, 175,\n",
       "       173,  85, 189, 208, 132, 114, 212,  72, 158, 100, 181,  31, 135,\n",
       "        76, 185, 167,  97, 125,  70,  88, 160, 200, 149, 129,  73,  42,\n",
       "       101, 191,  30, 113,  82, 161, 170,  18,  93,  61,  66, 184, 110,\n",
       "       196, 217, 131, 106, 163,  71, 165,  96, 127,  86,  92, 157,  91,\n",
       "       107, 201, 211, 123, 205, 213, 214, 215, 216, 209, 207, 199, 183,\n",
       "       203, 202, 198, 197, 194, 193, 192,  32,  33,  53, 179, 174,  54,\n",
       "       128, 134, 136, 137, 138, 176, 140, 142, 143, 144, 146, 148, 150,\n",
       "       151, 153, 154, 156, 159, 162, 166, 168, 169, 171, 172, 133, 126,\n",
       "       178, 124,  60,  63,  64,  67,  69,  74,  78,  80,  83,  84,  98,\n",
       "       105, 219, 111, 112, 115, 116, 117, 118, 119, 120, 121, 122,  87])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sorted_indices = np.argsort(importance)[::-1]\n",
    "\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Dictionary: \n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "Dict = {}\n",
    "print(\"Empty Dictionary: \")\n",
    "print(Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136960.601807</th>\n",
       "      <td>136960.601807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135492.398731</th>\n",
       "      <td>135492.398731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106326.197955</th>\n",
       "      <td>106326.197955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78280.684021</th>\n",
       "      <td>78280.684021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63900.928961</th>\n",
       "      <td>63900.928961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-36764.324272</th>\n",
       "      <td>-36764.324272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-45516.380532</th>\n",
       "      <td>-45516.380532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-64876.001927</th>\n",
       "      <td>-64876.001927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-194699.754152</th>\n",
       "      <td>-194699.754152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-584554.934452</th>\n",
       "      <td>-584554.934452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   importance\n",
       " 136960.601807  136960.601807\n",
       " 135492.398731  135492.398731\n",
       " 106326.197955  106326.197955\n",
       " 78280.684021    78280.684021\n",
       " 63900.928961    63900.928961\n",
       "...                       ...\n",
       "-36764.324272   -36764.324272\n",
       "-45516.380532   -45516.380532\n",
       "-64876.001927   -64876.001927\n",
       "-194699.754152 -194699.754152\n",
       "-584554.934452 -584554.934452\n",
       "\n",
       "[221 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of features: done\n",
    "feature_list = list(importance)\n",
    "\n",
    "# Save the results inside a DataFrame using feature_list as an index\n",
    "relative_importances = pd.DataFrame(index=feature_list, data=importance, columns=[\"importance\"])\n",
    "\n",
    "# Sort the DataFrame to learn most important features\n",
    "relative_importances.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.00132\n",
      "Feature: 1, Score: 0.00055\n",
      "Feature: 2, Score: 0.01207\n",
      "Feature: 3, Score: 0.00778\n",
      "Feature: 4, Score: 0.63603\n",
      "Feature: 5, Score: 0.00298\n",
      "Feature: 6, Score: 0.01233\n",
      "Feature: 7, Score: 0.00390\n",
      "Feature: 8, Score: 0.00239\n",
      "Feature: 9, Score: 0.02391\n",
      "Feature: 10, Score: 0.00062\n",
      "Feature: 11, Score: 0.00739\n",
      "Feature: 12, Score: 0.02841\n",
      "Feature: 13, Score: 0.01875\n",
      "Feature: 14, Score: 0.03714\n",
      "Feature: 15, Score: 0.00013\n",
      "Feature: 16, Score: 0.10126\n",
      "Feature: 17, Score: 0.00039\n",
      "Feature: 18, Score: 0.00000\n",
      "Feature: 19, Score: 0.00266\n",
      "Feature: 20, Score: 0.00024\n",
      "Feature: 21, Score: 0.00185\n",
      "Feature: 22, Score: 0.00580\n",
      "Feature: 23, Score: 0.00321\n",
      "Feature: 24, Score: 0.00035\n",
      "Feature: 25, Score: 0.02298\n",
      "Feature: 26, Score: 0.00878\n",
      "Feature: 27, Score: 0.00392\n",
      "Feature: 28, Score: 0.00169\n",
      "Feature: 29, Score: 0.00086\n",
      "Feature: 30, Score: 0.00000\n",
      "Feature: 31, Score: 0.00003\n",
      "Feature: 32, Score: 0.00000\n",
      "Feature: 33, Score: 0.00000\n",
      "Feature: 34, Score: 0.00472\n",
      "Feature: 35, Score: 0.00027\n",
      "Feature: 36, Score: 0.00251\n",
      "Feature: 37, Score: 0.00011\n",
      "Feature: 38, Score: 0.00075\n",
      "Feature: 39, Score: 0.00009\n",
      "Feature: 40, Score: 0.00199\n",
      "Feature: 41, Score: 0.00142\n",
      "Feature: 42, Score: 0.00001\n",
      "Feature: 43, Score: 0.00070\n",
      "Feature: 44, Score: 0.00261\n",
      "Feature: 45, Score: 0.00089\n",
      "Feature: 46, Score: 0.00024\n",
      "Feature: 47, Score: 0.00198\n",
      "Feature: 48, Score: 0.00047\n",
      "Feature: 49, Score: 0.00111\n",
      "Feature: 50, Score: 0.00040\n",
      "Feature: 51, Score: 0.00639\n",
      "Feature: 52, Score: 0.00058\n",
      "Feature: 53, Score: 0.00000\n",
      "Feature: 54, Score: 0.00000\n",
      "Feature: 55, Score: 0.00021\n",
      "Feature: 56, Score: 0.00031\n",
      "Feature: 57, Score: 0.00007\n",
      "Feature: 58, Score: 0.00084\n",
      "Feature: 59, Score: 0.00009\n",
      "Feature: 60, Score: 0.00000\n",
      "Feature: 61, Score: 0.00000\n",
      "Feature: 62, Score: 0.00045\n",
      "Feature: 63, Score: 0.00000\n",
      "Feature: 64, Score: 0.00000\n",
      "Feature: 65, Score: 0.00045\n",
      "Feature: 66, Score: 0.00000\n",
      "Feature: 67, Score: 0.00000\n",
      "Feature: 68, Score: 0.00018\n",
      "Feature: 69, Score: 0.00000\n",
      "Feature: 70, Score: 0.00001\n",
      "Feature: 71, Score: 0.00000\n",
      "Feature: 72, Score: 0.00004\n",
      "Feature: 73, Score: 0.00001\n",
      "Feature: 74, Score: 0.00000\n",
      "Feature: 75, Score: 0.00029\n",
      "Feature: 76, Score: 0.00002\n",
      "Feature: 77, Score: 0.00043\n",
      "Feature: 78, Score: 0.00000\n",
      "Feature: 79, Score: 0.00013\n",
      "Feature: 80, Score: 0.00000\n",
      "Feature: 81, Score: 0.00020\n",
      "Feature: 82, Score: 0.00000\n",
      "Feature: 83, Score: 0.00000\n",
      "Feature: 84, Score: 0.00000\n",
      "Feature: 85, Score: 0.00005\n",
      "Feature: 86, Score: 0.00000\n",
      "Feature: 87, Score: 0.00000\n",
      "Feature: 88, Score: 0.00001\n",
      "Feature: 89, Score: 0.00023\n",
      "Feature: 90, Score: 0.00029\n",
      "Feature: 91, Score: 0.00000\n",
      "Feature: 92, Score: 0.00000\n",
      "Feature: 93, Score: 0.00000\n",
      "Feature: 94, Score: 0.00006\n",
      "Feature: 95, Score: 0.00064\n",
      "Feature: 96, Score: 0.00000\n",
      "Feature: 97, Score: 0.00002\n",
      "Feature: 98, Score: 0.00000\n",
      "Feature: 99, Score: 0.00014\n",
      "Feature: 100, Score: 0.00004\n",
      "Feature: 101, Score: 0.00001\n",
      "Feature: 102, Score: 0.00021\n",
      "Feature: 103, Score: 0.00014\n",
      "Feature: 104, Score: 0.00022\n",
      "Feature: 105, Score: 0.00000\n",
      "Feature: 106, Score: 0.00000\n",
      "Feature: 107, Score: 0.00000\n",
      "Feature: 108, Score: 0.00039\n",
      "Feature: 109, Score: 0.00027\n",
      "Feature: 110, Score: 0.00000\n",
      "Feature: 111, Score: 0.00000\n",
      "Feature: 112, Score: 0.00000\n",
      "Feature: 113, Score: 0.00000\n",
      "Feature: 114, Score: 0.00004\n",
      "Feature: 115, Score: 0.00000\n",
      "Feature: 116, Score: 0.00000\n",
      "Feature: 117, Score: 0.00000\n",
      "Feature: 118, Score: 0.00000\n",
      "Feature: 119, Score: 0.00000\n",
      "Feature: 120, Score: 0.00000\n",
      "Feature: 121, Score: 0.00000\n",
      "Feature: 122, Score: 0.00000\n",
      "Feature: 123, Score: 0.00000\n",
      "Feature: 124, Score: 0.00000\n",
      "Feature: 125, Score: 0.00001\n",
      "Feature: 126, Score: 0.00000\n",
      "Feature: 127, Score: 0.00000\n",
      "Feature: 128, Score: 0.00000\n",
      "Feature: 129, Score: 0.00001\n",
      "Feature: 130, Score: 0.00028\n",
      "Feature: 131, Score: 0.00000\n",
      "Feature: 132, Score: 0.00004\n",
      "Feature: 133, Score: 0.00000\n",
      "Feature: 134, Score: 0.00000\n",
      "Feature: 135, Score: 0.00002\n",
      "Feature: 136, Score: 0.00000\n",
      "Feature: 137, Score: 0.00000\n",
      "Feature: 138, Score: 0.00000\n",
      "Feature: 139, Score: 0.00052\n",
      "Feature: 140, Score: 0.00000\n",
      "Feature: 141, Score: 0.00016\n",
      "Feature: 142, Score: 0.00000\n",
      "Feature: 143, Score: 0.00000\n",
      "Feature: 144, Score: 0.00000\n",
      "Feature: 145, Score: 0.00007\n",
      "Feature: 146, Score: 0.00000\n",
      "Feature: 147, Score: 0.00005\n",
      "Feature: 148, Score: 0.00000\n",
      "Feature: 149, Score: 0.00001\n",
      "Feature: 150, Score: 0.00000\n",
      "Feature: 151, Score: 0.00000\n",
      "Feature: 152, Score: 0.00007\n",
      "Feature: 153, Score: 0.00000\n",
      "Feature: 154, Score: 0.00000\n",
      "Feature: 155, Score: 0.00033\n",
      "Feature: 156, Score: 0.00000\n",
      "Feature: 157, Score: 0.00000\n",
      "Feature: 158, Score: 0.00004\n",
      "Feature: 159, Score: 0.00000\n",
      "Feature: 160, Score: 0.00001\n",
      "Feature: 161, Score: 0.00000\n",
      "Feature: 162, Score: 0.00000\n",
      "Feature: 163, Score: 0.00000\n",
      "Feature: 164, Score: 0.00248\n",
      "Feature: 165, Score: 0.00000\n",
      "Feature: 166, Score: 0.00000\n",
      "Feature: 167, Score: 0.00002\n",
      "Feature: 168, Score: 0.00000\n",
      "Feature: 169, Score: 0.00000\n",
      "Feature: 170, Score: 0.00000\n",
      "Feature: 171, Score: 0.00000\n",
      "Feature: 172, Score: 0.00000\n",
      "Feature: 173, Score: 0.00005\n",
      "Feature: 174, Score: 0.00000\n",
      "Feature: 175, Score: 0.00005\n",
      "Feature: 176, Score: 0.00000\n",
      "Feature: 177, Score: 0.00023\n",
      "Feature: 178, Score: 0.00000\n",
      "Feature: 179, Score: 0.00000\n",
      "Feature: 180, Score: 0.00012\n",
      "Feature: 181, Score: 0.00003\n",
      "Feature: 182, Score: 0.00006\n",
      "Feature: 183, Score: 0.00000\n",
      "Feature: 184, Score: 0.00000\n",
      "Feature: 185, Score: 0.00002\n",
      "Feature: 186, Score: 0.00027\n",
      "Feature: 187, Score: 0.00040\n",
      "Feature: 188, Score: 0.00008\n",
      "Feature: 189, Score: 0.00004\n",
      "Feature: 190, Score: 0.00012\n",
      "Feature: 191, Score: 0.00000\n",
      "Feature: 192, Score: 0.00000\n",
      "Feature: 193, Score: 0.00000\n",
      "Feature: 194, Score: 0.00000\n",
      "Feature: 195, Score: 0.00188\n",
      "Feature: 196, Score: 0.00000\n",
      "Feature: 197, Score: 0.00000\n",
      "Feature: 198, Score: 0.00000\n",
      "Feature: 199, Score: 0.00000\n",
      "Feature: 200, Score: 0.00001\n",
      "Feature: 201, Score: 0.00000\n",
      "Feature: 202, Score: 0.00000\n",
      "Feature: 203, Score: 0.00000\n",
      "Feature: 204, Score: 0.00014\n",
      "Feature: 205, Score: 0.00000\n",
      "Feature: 206, Score: 0.00009\n",
      "Feature: 207, Score: 0.00000\n",
      "Feature: 208, Score: 0.00004\n",
      "Feature: 209, Score: 0.00000\n",
      "Feature: 210, Score: 0.00468\n",
      "Feature: 211, Score: 0.00000\n",
      "Feature: 212, Score: 0.00004\n",
      "Feature: 213, Score: 0.00000\n",
      "Feature: 214, Score: 0.00000\n",
      "Feature: 215, Score: 0.00000\n",
      "Feature: 216, Score: 0.00000\n",
      "Feature: 217, Score: 0.00000\n",
      "Feature: 218, Score: 0.00353\n",
      "Feature: 219, Score: 0.00000\n",
      "Feature: 220, Score: 0.00047\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# define the model\n",
    "model = DecisionTreeRegressor()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)Select all possible 2 pairs of these top 10 predictors, and train 45 linear models, list the scores and RMSE errors achieved by the top 10 predictors by score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 =[ 4,  16,  14,  12,   9,  25,  13,   6,   2,  26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = it.combinations(top10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16) <class 'tuple'>\n",
      "(4, 14) <class 'tuple'>\n",
      "(4, 12) <class 'tuple'>\n",
      "(4, 9) <class 'tuple'>\n",
      "(4, 25) <class 'tuple'>\n",
      "(4, 13) <class 'tuple'>\n",
      "(4, 6) <class 'tuple'>\n",
      "(4, 2) <class 'tuple'>\n",
      "(4, 26) <class 'tuple'>\n",
      "(16, 14) <class 'tuple'>\n",
      "(16, 12) <class 'tuple'>\n",
      "(16, 9) <class 'tuple'>\n",
      "(16, 25) <class 'tuple'>\n",
      "(16, 13) <class 'tuple'>\n",
      "(16, 6) <class 'tuple'>\n",
      "(16, 2) <class 'tuple'>\n",
      "(16, 26) <class 'tuple'>\n",
      "(14, 12) <class 'tuple'>\n",
      "(14, 9) <class 'tuple'>\n",
      "(14, 25) <class 'tuple'>\n",
      "(14, 13) <class 'tuple'>\n",
      "(14, 6) <class 'tuple'>\n",
      "(14, 2) <class 'tuple'>\n",
      "(14, 26) <class 'tuple'>\n",
      "(12, 9) <class 'tuple'>\n",
      "(12, 25) <class 'tuple'>\n",
      "(12, 13) <class 'tuple'>\n",
      "(12, 6) <class 'tuple'>\n",
      "(12, 2) <class 'tuple'>\n",
      "(12, 26) <class 'tuple'>\n",
      "(9, 25) <class 'tuple'>\n",
      "(9, 13) <class 'tuple'>\n",
      "(9, 6) <class 'tuple'>\n",
      "(9, 2) <class 'tuple'>\n",
      "(9, 26) <class 'tuple'>\n",
      "(25, 13) <class 'tuple'>\n",
      "(25, 6) <class 'tuple'>\n",
      "(25, 2) <class 'tuple'>\n",
      "(25, 26) <class 'tuple'>\n",
      "(13, 6) <class 'tuple'>\n",
      "(13, 2) <class 'tuple'>\n",
      "(13, 26) <class 'tuple'>\n",
      "(6, 2) <class 'tuple'>\n",
      "(6, 26) <class 'tuple'>\n",
      "(2, 26) <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "for i in combinations:\n",
    "    print(i, type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [(4,16)]\n",
    "\n",
    "target = 'SalePrice'\n",
    "\n",
    "X = housing_ml[features].values.reshape(-1, len(features))\n",
    "\n",
    "\n",
    "model = linear_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)Train a single model using all features. Calculate RMSE and score. Observe how much of the prediction power was in the 2 pairs, vs all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.003829\n",
      "Feature 1: 0.276598\n",
      "Feature 2: 0.164263\n",
      "Feature 3: 0.161541\n",
      "Feature 4: 0.516781\n",
      "Feature 5: 0.092374\n",
      "Feature 6: 0.360999\n",
      "Feature 7: 0.241032\n",
      "Feature 8: 0.082815\n",
      "Feature 9: 0.169728\n",
      "Feature 10: 0.011823\n",
      "Feature 11: 0.136570\n",
      "Feature 12: 0.360355\n",
      "Feature 13: 0.297991\n",
      "Feature 14: 0.205934\n",
      "Feature 15: 0.002768\n",
      "Feature 16: 0.467822\n",
      "Feature 17: 0.023380\n",
      "Feature 18: 0.014285\n",
      "Feature 19: 0.273111\n",
      "Feature 20: 0.082441\n",
      "Feature 21: 0.060204\n",
      "Feature 22: 0.020151\n",
      "Feature 23: 0.225751\n",
      "Feature 24: 0.163579\n",
      "Feature 25: 0.377579\n",
      "Feature 26: 0.373009\n",
      "Feature 27: 0.096832\n",
      "Feature 28: 0.147105\n",
      "Feature 29: 0.024940\n",
      "Feature 30: 0.000000\n",
      "Feature 31: 0.015418\n",
      "Feature 32: 0.000000\n",
      "Feature 33: 0.014071\n",
      "Feature 34: 0.000000\n",
      "Feature 35: 0.020270\n",
      "Feature 36: 0.309540\n",
      "Feature 37: 0.009028\n",
      "Feature 38: 0.324100\n",
      "Feature 39: 0.042887\n",
      "Feature 40: 0.088292\n",
      "Feature 41: 0.141790\n",
      "Feature 42: 0.033581\n",
      "Feature 43: 0.162118\n",
      "Feature 44: 0.325650\n",
      "Feature 45: 0.028132\n",
      "Feature 46: 0.210571\n",
      "Feature 47: 0.264917\n",
      "Feature 48: 0.072774\n",
      "Feature 49: 0.073093\n",
      "Feature 50: 0.053577\n",
      "Feature 51: 0.053558\n",
      "Feature 52: 0.004629\n",
      "Feature 53: 0.003505\n",
      "Feature 54: 0.010523\n",
      "Feature 55: 0.000000\n",
      "Feature 56: 0.013341\n",
      "Feature 57: 0.060822\n",
      "Feature 58: 0.008164\n",
      "Feature 59: 0.026554\n",
      "Feature 60: 0.000000\n",
      "Feature 61: 0.026941\n",
      "Feature 62: 0.071182\n",
      "Feature 63: 0.000000\n",
      "Feature 64: 0.015049\n",
      "Feature 65: 0.052648\n",
      "Feature 66: 0.005762\n",
      "Feature 67: 0.003668\n",
      "Feature 68: 0.063496\n",
      "Feature 69: 0.013148\n",
      "Feature 70: 0.008717\n",
      "Feature 71: 0.001899\n",
      "Feature 72: 0.000000\n",
      "Feature 73: 0.000000\n",
      "Feature 74: 0.000000\n",
      "Feature 75: 0.000000\n",
      "Feature 76: 0.024681\n",
      "Feature 77: 0.005758\n",
      "Feature 78: 0.000000\n",
      "Feature 79: 0.002787\n",
      "Feature 80: 0.009239\n",
      "Feature 81: 0.000000\n",
      "Feature 82: 0.000000\n",
      "Feature 83: 0.008900\n",
      "Feature 84: 0.001973\n",
      "Feature 85: 0.000000\n",
      "Feature 86: 0.016498\n",
      "Feature 87: 0.010548\n",
      "Feature 88: 0.053009\n",
      "Feature 89: 0.009304\n",
      "Feature 90: 0.052042\n",
      "Feature 91: 0.029863\n",
      "Feature 92: 0.039072\n",
      "Feature 93: 0.002371\n",
      "Feature 94: 0.015060\n",
      "Feature 95: 0.064500\n",
      "Feature 96: 0.007999\n",
      "Feature 97: 0.038271\n",
      "Feature 98: 0.044684\n",
      "Feature 99: 0.053864\n",
      "Feature 100: 0.032741\n",
      "Feature 101: 0.000000\n",
      "Feature 102: 0.021689\n",
      "Feature 103: 0.014116\n",
      "Feature 104: 0.046719\n",
      "Feature 105: 0.012843\n",
      "Feature 106: 0.006297\n",
      "Feature 107: 0.004805\n",
      "Feature 108: 0.013232\n",
      "Feature 109: 0.003109\n",
      "Feature 110: 0.009798\n",
      "Feature 111: 0.011766\n",
      "Feature 112: 0.000000\n",
      "Feature 113: 0.000000\n",
      "Feature 114: 0.000000\n",
      "Feature 115: 0.000000\n",
      "Feature 116: 0.000000\n",
      "Feature 117: 0.005235\n",
      "Feature 118: 0.000000\n",
      "Feature 119: 0.011135\n",
      "Feature 120: 0.004954\n",
      "Feature 121: 0.000000\n",
      "Feature 122: 0.005656\n",
      "Feature 123: 0.008450\n",
      "Feature 124: 0.015992\n",
      "Feature 125: 0.009360\n",
      "Feature 126: 0.007015\n",
      "Feature 127: 0.021722\n",
      "Feature 128: 0.003813\n",
      "Feature 129: 0.000000\n",
      "Feature 130: 0.029366\n",
      "Feature 131: 0.004300\n",
      "Feature 132: 0.000000\n",
      "Feature 133: 0.000000\n",
      "Feature 134: 0.000000\n",
      "Feature 135: 0.037890\n",
      "Feature 136: 0.008411\n",
      "Feature 137: 0.018866\n",
      "Feature 138: 0.000000\n",
      "Feature 139: 0.028486\n",
      "Feature 140: 0.000000\n",
      "Feature 141: 0.024871\n",
      "Feature 142: 0.000000\n",
      "Feature 143: 0.008869\n",
      "Feature 144: 0.000000\n",
      "Feature 145: 0.000000\n",
      "Feature 146: 0.000000\n",
      "Feature 147: 0.008170\n",
      "Feature 148: 0.007344\n",
      "Feature 149: 0.007611\n",
      "Feature 150: 0.008199\n",
      "Feature 151: 0.007396\n",
      "Feature 152: 0.006656\n",
      "Feature 153: 0.000000\n",
      "Feature 154: 0.000000\n",
      "Feature 155: 0.000000\n",
      "Feature 156: 0.000000\n",
      "Feature 157: 0.023378\n",
      "Feature 158: 0.018652\n",
      "Feature 159: 0.000000\n",
      "Feature 160: 0.028680\n",
      "Feature 161: 0.023308\n",
      "Feature 162: 0.002463\n",
      "Feature 163: 0.003700\n",
      "Feature 164: 0.082010\n",
      "Feature 165: 0.025870\n",
      "Feature 166: 0.000000\n",
      "Feature 167: 0.013898\n",
      "Feature 168: 0.000000\n",
      "Feature 169: 0.010886\n",
      "Feature 170: 0.000000\n",
      "Feature 171: 0.006269\n",
      "Feature 172: 0.023307\n",
      "Feature 173: 0.020985\n",
      "Feature 174: 0.002415\n",
      "Feature 175: 0.033741\n",
      "Feature 176: 0.005786\n",
      "Feature 177: 0.019884\n",
      "Feature 178: 0.000000\n",
      "Feature 179: 0.005072\n",
      "Feature 180: 0.092935\n",
      "Feature 181: 0.024873\n",
      "Feature 182: 0.010018\n",
      "Feature 183: 0.005700\n",
      "Feature 184: 0.003635\n",
      "Feature 185: 0.000277\n",
      "Feature 186: 0.060726\n",
      "Feature 187: 0.045927\n",
      "Feature 188: 0.033094\n",
      "Feature 189: 0.098447\n",
      "Feature 190: 0.187412\n",
      "Feature 191: 0.022667\n",
      "Feature 192: 0.007018\n",
      "Feature 193: 0.000000\n",
      "Feature 194: 0.001847\n",
      "Feature 195: 0.027598\n",
      "Feature 196: 0.000000\n",
      "Feature 197: 0.012313\n",
      "Feature 198: 0.000000\n",
      "Feature 199: 0.000000\n",
      "Feature 200: 0.031077\n",
      "Feature 201: 0.006509\n",
      "Feature 202: 0.000000\n",
      "Feature 203: 0.005080\n",
      "Feature 204: 0.047110\n",
      "Feature 205: 0.003347\n",
      "Feature 206: 0.117632\n",
      "Feature 207: 0.000000\n",
      "Feature 208: 0.027750\n",
      "Feature 209: 0.017721\n",
      "Feature 210: 0.085763\n",
      "Feature 211: 0.062176\n",
      "Feature 212: 0.000596\n",
      "Feature 213: 0.013156\n",
      "Feature 214: 0.002073\n",
      "Feature 215: 0.000000\n",
      "Feature 216: 0.003340\n",
      "Feature 217: 0.000000\n",
      "Feature 218: 0.057416\n",
      "Feature 219: 0.010089\n",
      "Feature 220: 0.026439\n"
     ]
    }
   ],
   "source": [
    "# example of mutual information feature selection for numerical input data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "\t# configure to select all features\n",
    "\tfs = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "\t# learn relationship from training data\n",
    "\tfs.fit(X_train, y_train)\n",
    "\t# transform train input data\n",
    "\tX_train_fs = fs.transform(X_train)\n",
    "\t# transform test input data\n",
    "\tX_test_fs = fs.transform(X_test)\n",
    "\treturn X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "\tprint('Feature %d: %f' % (i, fs.scores_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)Use the 5NN and 10NN regressor with all features, and list the RMSE and score for these 2 models \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html \n",
    "observe if the results are better than linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor()"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    ">>> neigh = KNeighborsRegressor(n_neighbors=5)\n",
    ">>> neigh.fit(X_train, y_train)\n",
    "KNeighborsRegressor()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=10)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    ">>> neigh = NearestNeighbors(n_neighbors=10)\n",
    ">>> neigh.fit(X_train, y_train)\n",
    "NearestNeighbors(n_neighbors=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)Which regressor is better for inference? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN would be better for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
